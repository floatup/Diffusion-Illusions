{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-scale overlay guidance (image + optional text prompts)\n",
    "\n",
    "This notebook adds a multi-scale CLIP loss: large scale keeps the global silhouette, small scale enriches texture/details."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import rp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import source.stable_diffusion as sd\n",
    "from easydict import EasyDict\n",
    "from source.learnable_textures import LearnableImageFourier\n",
    "from source.stable_diffusion_labels import NegativeLabel\n",
    "from itertools import chain\n",
    "from source.clip import get_clip_image_similarity, get_clip_logits\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Image prompts (required)\n",
    "image_prompt_a = rp.load_image('source/GI.jpg')\n",
    "image_prompt_b = rp.load_image('source/HI.jpg')\n",
    "image_prompt_c = rp.load_image('source/HSR.jpg')\n",
    "image_prompt_d = rp.load_image('source/zzz.jpg')\n",
    "\n",
    "image_prompt_a = rp.as_rgb_image(rp.as_float_image(image_prompt_a))\n",
    "image_prompt_b = rp.as_rgb_image(rp.as_float_image(image_prompt_b))\n",
    "image_prompt_c = rp.as_rgb_image(rp.as_float_image(image_prompt_c))\n",
    "image_prompt_d = rp.as_rgb_image(rp.as_float_image(image_prompt_d))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Text prompts (optional). Leave '' to disable.\n",
    "text_prompt_w = ''\n",
    "text_prompt_x = ''\n",
    "text_prompt_y = ''\n",
    "text_prompt_z = ''\n",
    "\n",
    "text_prompt_weight = 0.5  # 0 disables text guidance\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Multi-scale loss configuration\n",
    "# Use higher weights for larger scales to keep global structure, smaller for detail.\n",
    "multi_scales = [1.0, 0.5, 0.25]\n",
    "multi_scale_weights = [1.0, 0.5, 0.25]\n",
    "assert len(multi_scales) == len(multi_scale_weights)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "s = sd._get_stable_diffusion_singleton()\n",
    "DEVICE = s.device\n",
    "\n",
    "# Learnable texture setup\n",
    "bottom_image = LearnableImageFourier(512, 512, 3).to(DEVICE)\n",
    "top_image = LearnableImageFourier(512, 512, 3).to(DEVICE)\n",
    "\n",
    "def simulate_overlay(bottom, top, clean=True):\n",
    "    if clean:\n",
    "        exp=1\n",
    "        brightness=3\n",
    "        black=0\n",
    "    else:\n",
    "        exp=rp.random_float(.5,1)\n",
    "        brightness=rp.random_float(1,5)\n",
    "        black=rp.random_float(0,.5)\n",
    "        bottom=rp.blend(bottom,black,rp.random_float())\n",
    "        top=rp.blend(top,black,rp.random_float())\n",
    "    return (bottom**exp * top**exp * brightness).clamp(0,99).tanh()\n",
    "\n",
    "def bottom_image_torch():\n",
    "    return bottom_image()\n",
    "\n",
    "def top_image_torch():\n",
    "    return top_image()\n",
    "\n",
    "learnable_image_w = lambda: simulate_overlay(bottom_image_torch(), top_image_torch().rot90(k=0, dims=[1,2]))\n",
    "learnable_image_x = lambda: simulate_overlay(bottom_image_torch(), top_image_torch().rot90(k=1, dims=[1,2]))\n",
    "learnable_image_y = lambda: simulate_overlay(bottom_image_torch(), top_image_torch().rot90(k=2, dims=[1,2]))\n",
    "learnable_image_z = lambda: simulate_overlay(bottom_image_torch(), top_image_torch().rot90(k=3, dims=[1,2]))\n",
    "\n",
    "image_prompt_a = rp.as_torch_image(image_prompt_a).to(DEVICE)\n",
    "image_prompt_b = rp.as_torch_image(image_prompt_b).to(DEVICE)\n",
    "image_prompt_c = rp.as_torch_image(image_prompt_c).to(DEVICE)\n",
    "image_prompt_d = rp.as_torch_image(image_prompt_d).to(DEVICE)\n",
    "\n",
    "learnable_images = [learnable_image_w, learnable_image_x, learnable_image_y, learnable_image_z]\n",
    "weights = np.array([1,1,1,1], dtype=np.float32)\n",
    "weights = weights / weights.sum()\n",
    "weights = weights * len(weights)\n",
    "\n",
    "params = chain(bottom_image.parameters(), top_image.parameters())\n",
    "optim = torch.optim.SGD(params, lr=1e-3)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def multiscale_clip_loss(composite, prompt_image, text_prompt):\n",
    "    loss = 0.0\n",
    "    for scale, w in zip(multi_scales, multi_scale_weights):\n",
    "        if scale == 1.0:\n",
    "            comp_s = composite\n",
    "            prompt_s = prompt_image\n",
    "        else:\n",
    "            size = int(512 * scale)\n",
    "            comp_s = rp.torch_resize_image(composite, (size, size))\n",
    "            prompt_s = rp.torch_resize_image(prompt_image, (size, size))\n",
    "        loss = loss - w * get_clip_image_similarity(comp_s, prompt_s)\n",
    "    if text_prompt_weight and text_prompt:\n",
    "        text_logit = get_clip_logits(composite, text_prompt)[0]\n",
    "        loss = loss - text_prompt_weight * text_logit\n",
    "    return loss\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "NUM_ITER = 10000\n",
    "DISPLAY_INTERVAL = 200\n",
    "\n",
    "text_prompts = [text_prompt_w, text_prompt_x, text_prompt_y, text_prompt_z]\n",
    "\n",
    "ims = []\n",
    "display_eta = rp.eta(NUM_ITER, title='Status: ')\n",
    "\n",
    "try:\n",
    "    for iter_num in range(NUM_ITER):\n",
    "        display_eta(iter_num)\n",
    "        for learnable_image, weight, prompt_image, text_prompt in rp.random_batch(\n",
    "            list(zip(learnable_images, weights, [image_prompt_a, image_prompt_b, image_prompt_c, image_prompt_d], text_prompts)),\n",
    "            batch_size=1,\n",
    "        ):\n",
    "            composite = learnable_image()\n",
    "            loss = multiscale_clip_loss(composite, prompt_image, text_prompt)\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if iter_num and not iter_num % (DISPLAY_INTERVAL * 50):\n",
    "                from IPython.display import clear_output\n",
    "                clear_output()\n",
    "            if not iter_num % DISPLAY_INTERVAL:\n",
    "                # display overlay and components\n",
    "                img_w = learnable_image_w().detach().cpu()\n",
    "                img_x = learnable_image_x().detach().cpu()\n",
    "                img_y = learnable_image_y().detach().cpu()\n",
    "                img_z = learnable_image_z().detach().cpu()\n",
    "                bottom = bottom_image().detach().cpu()\n",
    "                top = top_image().detach().cpu()\n",
    "\n",
    "                im = rp.as_numpy_image(rp.grid_concatenated_images([\n",
    "                    [rp.as_numpy_image(img_w), rp.as_numpy_image(img_x), rp.as_numpy_image(img_y), rp.as_numpy_image(img_z)],\n",
    "                    [rp.as_numpy_image(bottom), rp.as_numpy_image(top)],\n",
    "                ]))\n",
    "                ims.append(im)\n",
    "                rp.display_image(im)\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "except KeyboardInterrupt:\n",
    "    print()\n",
    "    print(f'Interrupted early at iteration {iter_num}')\n",
    "    if ims:\n",
    "        rp.display_image(ims[-1])\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}