{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-scale single-image guidance\n",
    "\n",
    "Optimize a single learnable image with multi-scale CLIP loss (no overlay, no rotation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rp\n",
    "import torch\n",
    "import source.stable_diffusion as sd\n",
    "from source.learnable_textures import LearnableImageFourier\n",
    "from source.clip import get_clip_image_similarity, get_clip_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text prompt (optional). Leave '' to disable.\n",
    "prompt_a = ''\n",
    "prompt_b = ''\n",
    "prompt_c = ''\n",
    "text_prompt_weight = 0.5  # 0 disables text guidance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-scale loss configuration\n",
    "multi_scales = [1.0, 0.5, 0.25]\n",
    "multi_scale_weights = [1.0, 0.5, 0.25]\n",
    "assert len(multi_scales) == len(multi_scale_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sd._get_stable_diffusion_singleton()\n",
    "DEVICE = s.device\n",
    "\n",
    "learnable_image = LearnableImageFourier(512, 512, 3).to(DEVICE)\n",
    "\n",
    "optim = torch.optim.SGD(learnable_image.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiscale_clip_loss(composite, text_prompt):\n",
    "    loss = 0.0\n",
    "    for scale, w in zip(multi_scales, multi_scale_weights):\n",
    "        if scale == 1.0:\n",
    "            comp_s = composite\n",
    "        else:\n",
    "            size = int(512 * scale)\n",
    "            comp_s = rp.torch_resize_image(composite, (size, size))\n",
    "        if text_prompt:\n",
    "            text_logit = get_clip_logits(comp_s, text_prompt)[0]\n",
    "            loss = loss - w * text_prompt_weight * text_logit\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITER = 8000\n",
    "DISPLAY_INTERVAL = 200\n",
    "\n",
    "ims = []\n",
    "display_eta = rp.eta(NUM_ITER, title='Status: ')\n",
    "\n",
    "try:\n",
    "    for iter_num in range(NUM_ITER):\n",
    "        display_eta(iter_num)\n",
    "        composite = learnable_image()\n",
    "        loss = multiscale_clip_loss(composite, text_prompt)\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if iter_num and not iter_num % (DISPLAY_INTERVAL * 50):\n",
    "                from IPython.display import clear_output\n",
    "                clear_output()\n",
    "            if not iter_num % DISPLAY_INTERVAL:\n",
    "                img = composite.detach().cpu()\n",
    "                im = rp.as_numpy_image(img)\n",
    "                ims.append(im)\n",
    "                rp.display_image(im)\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "except KeyboardInterrupt:\n",
    "    print()\n",
    "    print(f'Interrupted early at iteration {iter_num}')\n",
    "    if ims:\n",
    "        rp.display_image(ims[-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
