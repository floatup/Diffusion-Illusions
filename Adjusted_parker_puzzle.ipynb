{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c77bdb",
   "metadata": {},
   "source": [
    "# The Parker Puzzle: Other Cut-up \n",
    "\n",
    "在当前尝试下，我们通过修改 UV 映射图来体现对原图的裁切-扭曲-拼接，总体代码框架沿用 `parker_puzzle_colab.ipynb` 的内容，但是 uv 的映射图由本地生成。首先明确 uv 映射图的含义：\n",
    "\n",
    "UV 映射图的一个三通道的 RGB 图片（保存为 `png` 格式），其各个通道含义为：\n",
    "\n",
    "- R：表示 u 坐标，范围是 [0, 1]；\n",
    "- G：表示 v 坐标，范围是 [0, 1]；\n",
    "- B：通常为 0。\n",
    "\n",
    "需要注意的是，为了保证储存在本仓库的文件，需要 clone 我的仓库而非原仓库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f3ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ ! -d \".git\" ]; then \n",
    "    rm -rf * .*; #Get rid of Colab's default junk files\n",
    "    git clone -b master https://github.com/lh314-pku/Diffusion-Illusions .\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df639625",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade -r requirements.txt\n",
    "%pip install rp --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f356d4",
   "metadata": {},
   "source": [
    "关于提示词的部分，该模型大概率未使用中文语料库训练过，还是采用英语。描述时对画面的主体、风格、环境都需要描述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET YOUR PROMPTS HERE\n",
    "\n",
    "prompt_a = \"A ceramic coffee mug with handle viewed from side and steam rising. Beautiful 4k photography unreal engine 3d \"\n",
    "prompt_b = \"A donut. A shiny frosted chocolate donut. Beautiful 4k photography unreal engine 3d\"\n",
    "\n",
    "\n",
    "#Optional: Specify what you DON'T want to see\n",
    "negative_prompt = 'blurry ugly'\n",
    "\n",
    "print()\n",
    "print('Negative prompt:',repr(negative_prompt))\n",
    "print()\n",
    "print('Chosen prompts:')\n",
    "print('    prompt_a =', repr(prompt_a)) #This will be right-side up\n",
    "print('    prompt_b =', repr(prompt_b)) #This will be upside-down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da34bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rp import *\n",
    "import numpy as np\n",
    "import rp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import source.stable_diffusion as sd\n",
    "from easydict import EasyDict\n",
    "from source.learnable_textures import LearnableImageFourier\n",
    "from source.stable_diffusion_labels import NegativeLabel\n",
    "from itertools import chain\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604e8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 's' not in dir():\n",
    "    #You can select the original Stable Diffusion 1.5 or some dreambooth of it\n",
    "    model_name=\"CompVis/stable-diffusion-v1-4\"\n",
    "    model_name=\"runwayml/stable-diffusion-v1-5\"\n",
    "    # model_name=\"nitrosocke/Arcane-Diffusion\"\n",
    "    gpu=rp.select_torch_device()\n",
    "    s=sd.StableDiffusion(gpu,model_name)\n",
    "device=s.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08026e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_a = NegativeLabel(prompt_a,negative_prompt)\n",
    "label_b = NegativeLabel(prompt_b,negative_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91d2ff6",
   "metadata": {},
   "source": [
    "这里是我觉得论文中最出彩的地方之一：图片的隐式表示。通过将图像视为 “坐标到颜色的映射” 延伸到用 FFN 来表示图像并对对其进行优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34eba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Parametrization and Initialization (this section takes vram)\n",
    "\n",
    "#Select Learnable Image Size (this has big VRAM implications!):\n",
    "#Note: We use implicit neural representations for better image quality\n",
    "#They're previously used in our paper \"TRITON: Neural Neural Textures make Sim2Real Consistent\" (see tritonpaper.github.io)\n",
    "# ... and that representation is based on Fourier Feature Networks (see bmild.github.io/fourfeat)\n",
    "learnable_image_maker = lambda: LearnableImageFourier(height=256,width=256,num_features=256,hidden_dim=256,scale=10).to(s.device);SIZE=256\n",
    "\n",
    "image=learnable_image_maker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc264ce7",
   "metadata": {},
   "source": [
    "这里的 `uv_map_b` 可以替换为不同的 UV 映射图。当前可以采用的有：\n",
    "\n",
    "- `voronoi_uv.png`：根据沃罗诺伊图对图像平面进行划分，并进行拉伸和拼接。\n",
    "- `tangram_uv.png`；依据七巧板分割，并重新拼接为爱心图像，\n",
    "\n",
    "这些 uv 图利用 python 生成，位于 `UVs/UV_map_generate.py`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#This is the puzzle Matt used in his video!\n",
    "uv_map_b = rp.load_image(\"UVs/tangram_uv.png\")\n",
    "uv_map_a = rp.get_identity_uv_map(*rp.get_image_dimensions(uv_map_b))\n",
    "\n",
    "rp.display_image(uv_map_a)\n",
    "rp.display_image(uv_map_b)\n",
    "\n",
    "learnable_image_a = lambda: rp.apply_uv_map(image(), uv_map_a)\n",
    "learnable_image_b = lambda: rp.apply_uv_map(image(), uv_map_b)\n",
    "\n",
    "optim=torch.optim.SGD(image.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b77c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[label_a,label_b]\n",
    "learnable_images=[learnable_image_a,learnable_image_b]\n",
    "\n",
    "#The weight coefficients for each prompt. For example, if we have [0,1], then only the upside-down mode will be optimized\n",
    "weights=[1,1]\n",
    "\n",
    "weights=rp.as_numpy_array(weights)\n",
    "weights=weights/weights.sum()\n",
    "weights=weights*len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36504d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For saving a timelapse\n",
    "ims=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86da94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_display_image():\n",
    "    return rp.tiled_images(\n",
    "        [\n",
    "            rp.as_numpy_image(learnable_image_a()),\n",
    "            rp.as_numpy_image(learnable_image_b()),\n",
    "        ],\n",
    "        length=len(learnable_images),\n",
    "        border_thickness=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITER=5000\n",
    "\n",
    "#Set the minimum and maximum noise timesteps for the dream loss (aka score distillation loss)\n",
    "s.max_step=MAX_STEP=990\n",
    "s.min_step=MIN_STEP=10 \n",
    "\n",
    "television = rp.JupyterDisplayChannel()\n",
    "television.display()\n",
    "\n",
    "display_eta=rp.eta(NUM_ITER, title='Status')\n",
    "\n",
    "DISPLAY_INTERVAL = 200\n",
    "\n",
    "print('Every %i iterations we display an image in the form [image_a, image_b], where'%DISPLAY_INTERVAL)\n",
    "print('    image_a = (the right-side up image)')\n",
    "print('    image_b = (image_a, but upside down)')\n",
    "print()\n",
    "print('Interrupt the kernel at any time to return the currently displayed image')\n",
    "print('You can run this cell again to resume training later on')\n",
    "print()\n",
    "print('Please expect this to take quite a while to get good images (especially on the slower Colab GPU\\'s)! The longer you wait the better they\\'ll be')\n",
    "\n",
    "try:\n",
    "    for iter_num in range(NUM_ITER):\n",
    "        display_eta(iter_num) #Print the remaining time\n",
    "\n",
    "        preds=[]\n",
    "        for label,learnable_image,weight in rp.random_batch(list(zip(labels,learnable_images,weights)), batch_size=1):\n",
    "            pred=s.train_step(\n",
    "                label.embedding,\n",
    "                learnable_image()[None],\n",
    "\n",
    "                #PRESETS (uncomment one):\n",
    "                noise_coef=.1*weight,guidance_scale=100,#10\n",
    "                # noise_coef=0,image_coef=-.01,guidance_scale=50,\n",
    "                # noise_coef=0,image_coef=-.005,guidance_scale=50,\n",
    "                # noise_coef=.1,image_coef=-.010,guidance_scale=50,\n",
    "                # noise_coef=.1,image_coef=-.005,guidance_scale=50,\n",
    "                # noise_coef=.1*weight, image_coef=-.005*weight, guidance_scale=50,\n",
    "            )\n",
    "            preds+=list(pred)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if iter_num and not iter_num%(DISPLAY_INTERVAL*50):\n",
    "                #Wipe the slate every 50 displays so they don't get cut off\n",
    "                from IPython.display import clear_output\n",
    "                clear_output()\n",
    "\n",
    "            if not iter_num%(DISPLAY_INTERVAL//4):\n",
    "                im = get_display_image()\n",
    "                ims.append(im)\n",
    "                television.update(im)\n",
    "                \n",
    "                if not iter_num%DISPLAY_INTERVAL:\n",
    "                    rp.display_image(im)\n",
    "\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "except KeyboardInterrupt:\n",
    "    print()\n",
    "    print('Interrupted early at iteration %i'%iter_num)\n",
    "    im = get_display_image()\n",
    "    ims.append(im)\n",
    "    rp.display_image(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207918e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unsolved Image:')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_a()))\n",
    "\n",
    "print('Solved Image:')\n",
    "rp.display_image(rp.as_numpy_image(learnable_image_b()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6204e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_run(name):\n",
    "    folder=\"untracked/parker_puzzle_runs/%s\"%name\n",
    "    if rp.path_exists(folder):\n",
    "        folder+='_%i'%time.time()\n",
    "    rp.make_directory(folder)\n",
    "    ims_names=['ims_%04i.png'%i for i in range(len(ims))]\n",
    "    with rp.SetCurrentDirectoryTemporarily(folder):\n",
    "        rp.save_images(ims,ims_names,show_progress=True)\n",
    "    print()\n",
    "    print('Saved timelapse to folder:',repr(folder))\n",
    "    \n",
    "save_run('-'.join([prompt_a,prompt_b])) #You can give it a good custom name if you want!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
